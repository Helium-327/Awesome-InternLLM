# `LMDeploy`éƒ¨ç½²

ğŸ°å®˜æ–¹æ•™ç¨‹æ–‡æ¡£ï¼šhttps://github.com/InternLM/Tutorial/tree/camp3/docs/L1/Demo

## `Cli Demo`éƒ¨ç½² `InternLM2-Chat-1.8B`

### ç¯å¢ƒé…ç½®ï¼š

```shell
# åˆ›å»ºç¯å¢ƒ
conda create -n demo python=3.10 -y
# æ¿€æ´»ç¯å¢ƒ
conda activate demo
# å®‰è£… torch
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia -y
```

```shell
# requirements.txt
transformers==4.38      # é¢„è®­ç»ƒå·¥å…·
sentencepiece==0.1.99   # å°†æ–‡æœ¬è½¬æ¢ä¸ºå­è¯æˆ–è¯ç‰‡æ®µ
einops==0.8.0           # å¼ äº®æ“ä½œçš„åº“ï¼Œé‡å¡‘ã€è½¬ç½®ã€å‹ç¼©ã€æ‰©å±•
protobuf==5.27.2        # æ•°æ®åºåˆ—åŒ–åè®®
accelerate==0.33.0      # ç®€åŒ–è®­ç»ƒè¿‡ç¨‹
streamlit==1.37.0       # å¿«é€Ÿæ„å»ºäº¤äº’å¼webåº”ç”¨
```

### ç¼–å†™ä»£ç 

```python
# cli_demo.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


model_name_or_path = "/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, device_map='cuda:0')
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='cuda:0')
model = model.eval()

system_prompt = """You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
"""

messages = [(system_prompt, '')]

print("=============Welcome to InternLM chatbot, type 'exit' to exit.=============")

while True:
    input_text = input("\nUser  >>> ")
    input_text = input_text.replace(' ', '')
    if input_text == "exit":
        break

    length = 0
    for response, _ in model.stream_chat(tokenizer, input_text, messages):
        if response is not None:
            print(response[length:], flush=True, end="")
            length = len(response)

```

### è¿è¡Œç»“æœ

![image-20240728125830755](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281258804.png)

## `Streamlit Web Demo` éƒ¨ç½² `InternLM2-Chat-1.8B` æ¨¡å‹

### ä¸‹è½½ä»“åº“

```shell
cd /root/demo
git clone https://github.com/InternLM/Tutorial.git

```

### å¼€å¯`streamlit`æœåŠ¡

```shell

## remote 
streamlit run /root/demo/Tutorial/tools/streamlit_demo.py --server.address 127.0.0.1 --server.port 6006

```



![image-20240728125853820](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281258866.png)

### æœ¬åœ°ç«¯å£æ˜ å°„

```shell
## local
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p ä½ çš„ ssh ç«¯å£å·
```



![image-20240728125904867](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281259902.png)

![image-20240728131400546](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281314640.png)

### ã€**TASK**ã€‘ ç”Ÿæˆ`300`å­—çš„æ•…äº‹

![image-20240728134405214](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281344318.png)

## `LMDeploy` éƒ¨ç½² `InternLM-XComposer2-VL-1.8B` æ¨¡å‹

### `InternLM-XComposer2`

> InternLM-XComposer2 æ˜¯ä¸€æ¬¾åŸºäº InternLM2 çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼Œå…¶æ“…é•¿è‡ªç”±å½¢å¼çš„æ–‡æœ¬å›¾åƒåˆæˆå’Œç†è§£ã€‚å…¶ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
>
> - è‡ªç”±å½¢å¼çš„äº¤é”™æ–‡æœ¬å›¾åƒåˆæˆï¼š`InternLM-XComposer2` å¯ä»¥æ ¹æ®å¤§çº²ã€è¯¦ç»†æ–‡æœ¬è¦æ±‚å’Œå‚è€ƒå›¾åƒç­‰ä¸åŒè¾“å…¥ï¼Œç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡ç›¸å…³ï¼Œå…·æœ‰äº¤é”™å›¾åƒå’Œæ–‡æœ¬çš„æ–‡ç« ï¼Œä»è€Œå®ç°é«˜åº¦å¯å®šåˆ¶çš„å†…å®¹åˆ›å»ºã€‚
> - å‡†ç¡®çš„è§†è§‰è¯­è¨€é—®é¢˜è§£å†³ï¼šInternLM-XComposer2 åŸºäºè‡ªç”±å½¢å¼çš„æŒ‡ä»¤å‡†ç¡®åœ°å¤„ç†å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰è¯­è¨€é—®ç­”ä»»åŠ¡ï¼Œåœ¨è¯†åˆ«ï¼Œæ„ŸçŸ¥ï¼Œè¯¦ç»†æ ‡ç­¾ï¼Œè§†è§‰æ¨ç†ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
> - ä»¤äººæƒŠå¹çš„æ€§èƒ½ï¼šåŸºäº InternLM2-7B çš„InternLM-XComposer2 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä½äºå¼€æºå¤šæ¨¡æ€æ¨¡å‹ç¬¬ä¸€æ¢¯é˜Ÿï¼Œè€Œä¸”åœ¨éƒ¨åˆ†åŸºå‡†æµ‹è¯•ä¸­ä¸ GPT-4V å’Œ Gemini Pro ç›¸å½“ç”šè‡³è¶…è¿‡å®ƒä»¬ã€‚

### `LMDeploy`

> LMDeploy æ˜¯ä¸€ä¸ªç”¨äºå‹ç¼©ã€éƒ¨ç½²å’ŒæœåŠ¡ LLM çš„å·¥å…·åŒ…ï¼Œç”± MMRazor å’Œ MMDeploy å›¢é˜Ÿå¼€å‘ã€‚å®ƒå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š
>
> - é«˜æ•ˆçš„æ¨ç†ï¼šLMDeploy é€šè¿‡å¼•å…¥æŒä¹…åŒ–æ‰¹å¤„ç†ã€å— KV ç¼“å­˜ã€åŠ¨æ€åˆ†å‰²ä¸èåˆã€å¼ é‡å¹¶è¡Œã€é«˜æ€§èƒ½ CUDA å†…æ ¸ç­‰å…³é”®æŠ€æœ¯ï¼Œæä¾›äº†æ¯” vLLM é«˜ 1.8 å€çš„æ¨ç†æ€§èƒ½ã€‚
> - æœ‰æ•ˆçš„é‡åŒ–ï¼šLMDeploy æ”¯æŒä»…æƒé‡é‡åŒ–å’Œ k/v é‡åŒ–ï¼Œ4bit æ¨ç†æ€§èƒ½æ˜¯ FP16 çš„ 2.4 å€ã€‚é‡åŒ–åæ¨¡å‹è´¨é‡å·²é€šè¿‡ OpenCompass è¯„ä¼°ç¡®è®¤ã€‚
> - è½»æ¾çš„åˆ†å‘ï¼šåˆ©ç”¨è¯·æ±‚åˆ†å‘æœåŠ¡ï¼ŒLMDeploy å¯ä»¥åœ¨å¤šå°æœºå™¨å’Œè®¾å¤‡ä¸Šè½»æ¾é«˜æ•ˆåœ°éƒ¨ç½²å¤šæ¨¡å‹æœåŠ¡ã€‚
> - äº¤äº’å¼æ¨ç†æ¨¡å¼ï¼šé€šè¿‡ç¼“å­˜å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­æ³¨æ„åŠ›çš„ k/vï¼Œæ¨ç†å¼•æ“è®°ä½å¯¹è¯å†å²ï¼Œä»è€Œé¿å…é‡å¤å¤„ç†å†å²ä¼šè¯ã€‚
> - ä¼˜ç§€çš„å…¼å®¹æ€§ï¼šLMDeployæ”¯æŒ KV Cache Quantï¼ŒAWQ å’Œè‡ªåŠ¨å‰ç¼€ç¼“å­˜åŒæ—¶ä½¿ç”¨ã€‚

### ç¯å¢ƒå®‰è£…

```shell
conda activate demo
pip install lmdeploy[all]==0.5.1
pip install timm==1.0.7
```

### ä¸€æ¡å‘½ä»¤éƒ¨ç½²

```shell
lmdeploy serve gradio /share/new_models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-1_8b --cache-max-entry-count 0.1
```

![image-20240728135359492](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281353589.png)

### ã€**TASK**ã€‘å›¾ç‰‡å¯¹è¯

![image-20240728135341618](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281353755.png)

## `LMDeploy` éƒ¨ç½² `InternVL2-2B` æ¨¡å‹

```shell
lmdeploy serve gradio /share/new_models/OpenGVLab/InternVL2-2B --cache-max-entry-count 0.1
```

### ã€**TASK**ã€‘å›¾ç‰‡å¯¹è¯

![image-20240728133740369](https://raw.githubusercontent.com/Helium-327/PicGo/main/win/markdown/202407281337515.png)

